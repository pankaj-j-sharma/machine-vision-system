{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras \n",
    "from keras import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from PIL import Image\n",
    "import matplotlib.image as mpg\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['font.family'] = 'sans-serif' \n",
    "plt.rcParams['font.serif'] = 'Ubuntu' \n",
    "plt.rcParams['font.monospace'] = 'Ubuntu Mono' \n",
    "plt.rcParams['font.size'] = 14 \n",
    "plt.rcParams['axes.labelsize'] = 12 \n",
    "plt.rcParams['axes.labelweight'] = 'bold' \n",
    "plt.rcParams['axes.titlesize'] = 12 \n",
    "plt.rcParams['xtick.labelsize'] = 12 \n",
    "plt.rcParams['ytick.labelsize'] = 12 \n",
    "plt.rcParams['legend.fontsize'] = 12 \n",
    "plt.rcParams['figure.titlesize'] = 12 \n",
    "plt.rcParams['image.cmap'] = 'jet' \n",
    "plt.rcParams['image.interpolation'] = 'none' \n",
    "plt.rcParams['figure.figsize'] = (10, 10\n",
    "                                 ) \n",
    "plt.rcParams['axes.grid']=True\n",
    "plt.rcParams['lines.linewidth'] = 2 \n",
    "plt.rcParams['lines.markersize'] = 8\n",
    "colors = ['xkcd:pale orange', 'xkcd:sea blue', 'xkcd:pale red', 'xkcd:sage green', 'xkcd:terra cotta', 'xkcd:dull purple', 'xkcd:teal', 'xkcd: goldenrod', 'xkcd:cadet blue',\n",
    "'xkcd:scarlet']\n",
    "bbox_props = dict(boxstyle=\"round,pad=0.3\", fc=colors[0], alpha=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('D://POC//ComputerVisionSolution//Surface_defects_detection//archive//age_gender.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data.sample(frac=1).reset_index().loc[0:1000]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Convert(string):\n",
    "    li = list(string.split(\" \"))\n",
    "    return li\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIXELS=[]\n",
    "for i in range(len(data)):\n",
    "    PIXELS.append(Convert(data.pixels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_PIXELS = []\n",
    "for p in range(len(PIXELS)):\n",
    "    new_pixels = []\n",
    "    for q in range(len(PIXELS[p])):\n",
    "        new_pixels.append(int(PIXELS[p][q]))\n",
    "    NEW_PIXELS.append(np.array(new_pixels).reshape((48,48,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.array(NEW_PIXELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['index','img_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data.gender,palette='plasma')\n",
    "plt.xticks([0,1],['Male','Female'])\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.pixels = NEW_PIXELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,5):\n",
    "    J = np.random.choice(np.arange(0,1000,1))\n",
    "    plt.subplot(2,2,i)\n",
    "    plt.title('Label : '+ str(data.gender[J]),fontsize=20)\n",
    "    plt.imshow(np.array(data.pixels[J]).reshape((48,48)),cmap='gray')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classifier = Sequential()\n",
    "size = 48\n",
    "# Step 1 - Convolution\n",
    "classifier.add(Conv2D(64, (2, 2), input_shape = (size,size, 1), activation = 'relu'))\n",
    "\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "classifier.add(Conv2D(64, (2, 2), input_shape = (size,size, 1), activation = 'relu'))\n",
    "\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "classifier.add(Flatten())\n",
    "\n",
    "#classifier.add(Dense(units = 32, activation = 'relu'))\n",
    "classifier.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the CNN\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(data.gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, test_images, train_labels, test_labels = train_test_split(\n",
    "    images, labels, test_size=0.15)\n",
    "\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "history = classifier.fit(train_images, train_labels, epochs=10, \n",
    "                validation_data=(test_images, test_labels),batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['accuracy']\n",
    "val_loss = history.history['val_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1,len(loss)+1,1),loss,color='navy', label = 'Accuracy')\n",
    "plt.plot(np.arange(1,len(loss)+1,1),val_loss,color='red',label='Validation Accuracy')\n",
    "plt.legend(fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(classifier.predict(test_images),'.',color='red',label='Predicted Probabilty')\n",
    "plt.plot(test_labels,'.',color='navy',label='Actual Labels')\n",
    "plt.xlabel('Instance Number')\n",
    "plt.ylabel('Probability')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = classifier.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision = []\n",
    "\n",
    "for p in predictions:\n",
    "    if p>=0.5:\n",
    "        decision.append(1)\n",
    "    else:\n",
    "        decision.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(confusion_matrix(decision,test_labels),cmap='plasma',annot=True,annot_kws={\"size\": 32})\n",
    "plt.xticks([0.50,1.50],['Male','Female'],fontsize=20)\n",
    "plt.yticks([0.50,1.50],['Male','Female'],fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_report = classification_report(decision,test_labels,\n",
    "                                   labels=[0,1],\n",
    "                                   target_names=['Male','Female'],\n",
    "                                   output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.subplot(2,1,1)\n",
    "sns.heatmap(pd.DataFrame(clf_report).iloc[:-2, :-3].T, annot=True)\n",
    "plt.subplot(2,1,2)\n",
    "sns.heatmap(pd.DataFrame(clf_report).iloc[:-2, 3:].T, annot=True)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ImagePreProcessing(image_path):\n",
    "    img = Image.open(image_path)\n",
    "    img = img.resize((48,48))\n",
    "    img_array = np.asarray(img)\n",
    "    baw_img = rgb2gray(img_array).astype(int)\n",
    "    final_img = baw_img.reshape((48,48,1))\n",
    "    return final_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ImagePreProcessing('test.JPG'),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_test = ImagePreProcessing('test.JPG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = 1-classifier.predict(np.array([image_test]))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"I'm a man, and the classifier says that I'm a man with probability %.2f\" %(prob*100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.save_model(classifier,'/Users/pierohmd/Desktop/archive/classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('/Users/pierohmd/Desktop/archive/classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install visualkeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import visualkeras\n",
    "visualkeras.layered_view(model=classifier, to_file ='GenderClassifier.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "# from keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from keras.utils import to_categorical\n",
    "from tqdm import tqdm\n",
    "from sqlite_db_connect import SQLiteConnect\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(SQLiteConnect):\n",
    "    def ___init__(self):\n",
    "        super.__init__()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database created and Successfully Connected to SQLite\n",
      "query-> SELECT FileName,FilePath,Tag from RESOURCE_DETAILS  rec [{'FileName': 'cast_def_0_65.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_def_0_65.jpeg', 'Tag': 'notdefected'}, {'FileName': 'cast_def_0_67.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_def_0_67.jpeg', 'Tag': 'defected'}, {'FileName': 'cast_def_0_85.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_def_0_85.jpeg', 'Tag': 'defected'}, {'FileName': 'cast_def_0_87.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_def_0_87.jpeg', 'Tag': 'notdefected'}, {'FileName': 'cast_def_0_88.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_def_0_88.jpeg', 'Tag': 'notdefected'}, {'FileName': 'cast_def_0_108.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_def_0_108.jpeg', 'Tag': 'defected'}, {'FileName': 'cast_def_0_112.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_def_0_112.jpeg', 'Tag': 'defected'}, {'FileName': 'cast_def_0_118.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_def_0_118.jpeg', 'Tag': 'defected'}, {'FileName': 'cast_def_0_138.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_def_0_138.jpeg', 'Tag': 'defected'}, {'FileName': 'cast_def_0_143.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_def_0_143.jpeg', 'Tag': 'notdefected'}, {'FileName': 'cast_def_0_144.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_def_0_144.jpeg', 'Tag': 'notdefected'}, {'FileName': 'cast_def_0_147.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_def_0_147.jpeg', 'Tag': 'notdefected'}, {'FileName': 'cast_def_0_148.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_def_0_148.jpeg', 'Tag': 'notdefected'}, {'FileName': 'cast_def_0_149.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_def_0_149.jpeg', 'Tag': 'notdefected'}, {'FileName': 'cast_def_0_150.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_def_0_150.jpeg', 'Tag': 'notdefected'}, {'FileName': 'cast_def_0_151.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_def_0_151.jpeg', 'Tag': 'defected'}, {'FileName': 'cast_def_0_154.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_def_0_154.jpeg', 'Tag': 'defected'}, {'FileName': 'cast_def_0_157.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_def_0_157.jpeg', 'Tag': 'defected'}, {'FileName': 'cast_def_0_158.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_def_0_158.jpeg', 'Tag': 'notdefected'}, {'FileName': 'cast_def_0_159.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_def_0_159.jpeg', 'Tag': 'notdefected'}, {'FileName': 'cast_def_0_160.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_def_0_160.jpeg', 'Tag': 'defected'}, {'FileName': 'cast_def_0_161.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_def_0_161.jpeg', 'Tag': 'notdefected'}, {'FileName': 'cast_def_0_162.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_def_0_162.jpeg', 'Tag': 'defected'}, {'FileName': 'cast_def_0_164.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_def_0_164.jpeg', 'Tag': 'defected'}, {'FileName': 'cast_def_0_174.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_def_0_174.jpeg', 'Tag': 'defected'}, {'FileName': 'cast_def_0_175.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_def_0_175.jpeg', 'Tag': 'defected'}, {'FileName': 'cast_def_0_176.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_def_0_176.jpeg', 'Tag': 'defected'}, {'FileName': 'cast_def_0_181.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_def_0_181.jpeg', 'Tag': 'defected'}, {'FileName': 'cast_def_0_182.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_def_0_182.jpeg', 'Tag': 'notdefected'}, {'FileName': 'cast_ok_0_10.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_ok_0_10.jpeg', 'Tag': 'notdefected'}, {'FileName': 'cast_ok_0_14.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_ok_0_14.jpeg', 'Tag': 'notdefected'}, {'FileName': 'cast_ok_0_16.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_ok_0_16.jpeg', 'Tag': 'notdefected'}, {'FileName': 'cast_ok_0_31.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_ok_0_31.jpeg', 'Tag': 'notdefected'}, {'FileName': 'cast_ok_0_45.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_ok_0_45.jpeg', 'Tag': 'defected'}, {'FileName': 'cast_ok_0_49.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_ok_0_49.jpeg', 'Tag': 'notdefected'}, {'FileName': 'cast_ok_0_62.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_ok_0_62.jpeg', 'Tag': 'defected'}, {'FileName': 'cast_ok_0_64.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_ok_0_64.jpeg', 'Tag': 'notdefected'}, {'FileName': 'cast_ok_0_235.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_ok_0_235.jpeg', 'Tag': 'notdefected'}, {'FileName': 'cast_ok_0_238.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_ok_0_238.jpeg', 'Tag': None}, {'FileName': 'cast_ok_0_239.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_ok_0_239.jpeg', 'Tag': 'notdefected'}, {'FileName': 'cast_ok_0_246.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_ok_0_246.jpeg', 'Tag': None}, {'FileName': 'cast_ok_0_259.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_ok_0_259.jpeg', 'Tag': 'notdefected'}, {'FileName': 'cast_ok_0_268.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_ok_0_268.jpeg', 'Tag': 'notdefected'}, {'FileName': 'cast_ok_0_504.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_ok_0_504.jpeg', 'Tag': 'defected'}, {'FileName': 'cast_ok_0_505.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_ok_0_505.jpeg', 'Tag': 'defected'}, {'FileName': 'cast_ok_0_525.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_ok_0_525.jpeg', 'Tag': 'notdefected'}, {'FileName': 'cast_ok_0_526.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_ok_0_526.jpeg', 'Tag': 'notdefected'}, {'FileName': 'cast_ok_0_863.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_ok_0_863.jpeg', 'Tag': 'notdefected'}, {'FileName': 'cast_ok_0_864.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_ok_0_864.jpeg', 'Tag': 'notdefected'}, {'FileName': 'cast_ok_0_869.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_ok_0_869.jpeg', 'Tag': 'notdefected'}, {'FileName': 'cast_ok_0_880.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_ok_0_880.jpeg', 'Tag': 'defected'}, {'FileName': 'cast_ok_0_897.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_ok_0_897.jpeg', 'Tag': 'notdefected'}, {'FileName': 'cast_ok_0_903.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_ok_0_903.jpeg', 'Tag': 'defected'}, {'FileName': 'cast_ok_0_904.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_ok_0_904.jpeg', 'Tag': 'notdefected'}, {'FileName': 'cast_ok_0_907.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_ok_0_907.jpeg', 'Tag': 'defected'}, {'FileName': 'cast_ok_0_925.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_ok_0_925.jpeg', 'Tag': 'notdefected'}, {'FileName': 'cast_ok_0_927.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_ok_0_927.jpeg', 'Tag': 'defected'}, {'FileName': 'cast_ok_0_932.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_ok_0_932.jpeg', 'Tag': 'notdefected'}, {'FileName': 'cast_ok_0_935.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_ok_0_935.jpeg', 'Tag': 'notdefected'}, {'FileName': 'cast_ok_0_937.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_ok_0_937.jpeg', 'Tag': 'notdefected'}, {'FileName': 'cast_ok_0_942.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_ok_0_942.jpeg', 'Tag': 'notdefected'}, {'FileName': 'cast_ok_0_952.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_ok_0_952.jpeg', 'Tag': 'notdefected'}, {'FileName': 'cast_ok_0_960.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_ok_0_960.jpeg', 'Tag': 'defected'}, {'FileName': 'cast_ok_0_973.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_ok_0_973.jpeg', 'Tag': 'defected'}, {'FileName': 'cast_ok_0_975.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_ok_0_975.jpeg', 'Tag': 'defected'}, {'FileName': 'cast_ok_0_976.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_ok_0_976.jpeg', 'Tag': 'defected'}, {'FileName': 'cast_ok_0_989.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_ok_0_989.jpeg', 'Tag': 'defected'}, {'FileName': 'cast_ok_0_998.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_ok_0_998.jpeg', 'Tag': 'defected'}, {'FileName': 'cast_ok_0_1001.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_ok_0_1001.jpeg', 'Tag': 'notdefected'}, {'FileName': 'cast_ok_0_1002.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_ok_0_1002.jpeg', 'Tag': 'defected'}, {'FileName': 'cast_ok_0_1003.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_ok_0_1003.jpeg', 'Tag': 'notdefected'}, {'FileName': 'cast_ok_0_1019.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_ok_0_1019.jpeg', 'Tag': 'notdefected'}, {'FileName': 'cast_ok_0_1020.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_ok_0_1020.jpeg', 'Tag': 'notdefected'}, {'FileName': 'cast_ok_0_1021.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_ok_0_1021.jpeg', 'Tag': 'defected'}, {'FileName': 'cast_ok_0_1022.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_ok_0_1022.jpeg', 'Tag': 'notdefected'}, {'FileName': 'cast_def_0_7.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_def_0_7.jpeg', 'Tag': 'notdefected'}, {'FileName': 'cast_def_0_15.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_def_0_15.jpeg', 'Tag': 'defected'}, {'FileName': 'cast_def_0_26.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_def_0_26.jpeg', 'Tag': 'defected'}, {'FileName': 'cast_def_0_28.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_def_0_28.jpeg', 'Tag': 'defected'}, {'FileName': 'cast_def_0_40.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_def_0_40.jpeg', 'Tag': 'defected'}, {'FileName': 'cast_def_0_49.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_def_0_49.jpeg', 'Tag': 'defected'}, {'FileName': 'cast_def_0_61.jpeg', 'FilePath': 'Projects\\\\1\\\\1\\\\cast_def_0_61.jpeg', 'Tag': 'defected'}]\n"
     ]
    }
   ],
   "source": [
    "classifierObj = Classifier()\n",
    "data = classifierObj.getRecords('SELECT FileName,FilePath,Tag from RESOURCE_DETAILS ')\n",
    "train = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 82/82 [00:00<00:00, 88.64it/s]\n"
     ]
    }
   ],
   "source": [
    "# We have grayscale images, so while loading the images we will keep grayscale=True, if you have RGB images, you should set grayscale as False\n",
    "image_shape=(100,100,1)\n",
    "train_image = []\n",
    "for i in tqdm(range(train.shape[0])):\n",
    "    img = image.load_img(os.path.join('uploads',train['FilePath'][i]), target_size=image_shape , color_mode = \"grayscale\")\n",
    "    img = image.img_to_array(img)\n",
    "    img = img/255\n",
    "    train_image.append(img)\n",
    "X = np.array(train_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "dy=train['Tag'].values\n",
    "vec = label_encoder.fit_transform(dy)\n",
    "y = to_categorical(vec)\n",
    "output_classes= len(set(vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = os.path.join(os.getcwd(),'uploads','User_Model_Files','mynewmodel.hdf5')\n",
    "early_stop = EarlyStopping(monitor='val_loss',patience=5)\n",
    "checkpoint = ModelCheckpoint(filepath=model_save_path, verbose=1, save_best_only=True, monitor='val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=image_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(output_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16, kernel_size=(7,7), strides=2, input_shape=image_shape, activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3), strides=1, input_shape=image_shape, activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), strides=1, input_shape=image_shape, activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=224, activation='relu'))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(Dense(units=output_classes, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_62 (Conv2D)           (None, 27, 27, 64)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_52 (MaxPooling (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_63 (Conv2D)           (None, 12, 12, 64)        16448     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_53 (MaxPooling (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_22 (Flatten)         (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 32)                73760     \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 90,627\n",
      "Trainable params: 90,627\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# Step 1 - Convolution\n",
    "model.add(Conv2D(64, (2, 2), input_shape = image_shape, activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Conv2D(64, (2, 2), input_shape = (size,size, 1), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units = 32, activation = 'relu'))\n",
    "model.add(Dense(units = output_classes, activation = 'sigmoid'))\n",
    "# Compiling the CNN\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "3/3 [==============================] - 2s 349ms/step - loss: 0.6830 - accuracy: 0.4301 - val_loss: 0.6310 - val_accuracy: 0.2353\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.39861\n",
      "Epoch 2/50\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.6127 - accuracy: 0.5428 - val_loss: 0.5529 - val_accuracy: 0.7647\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.39861\n",
      "Epoch 3/50\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.5621 - accuracy: 0.4692 - val_loss: 0.4912 - val_accuracy: 0.7647\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.39861\n",
      "Epoch 4/50\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.5227 - accuracy: 0.4341 - val_loss: 0.4787 - val_accuracy: 0.4706\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.39861\n",
      "Epoch 5/50\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.4991 - accuracy: 0.6048 - val_loss: 0.4505 - val_accuracy: 0.7647\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.39861\n",
      "Epoch 6/50\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.5147 - accuracy: 0.4418 - val_loss: 0.4084 - val_accuracy: 0.7647\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.39861\n",
      "Epoch 7/50\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.5368 - accuracy: 0.4770 - val_loss: 0.3979 - val_accuracy: 0.7647\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.39861 to 0.39789, saving model to D:\\POC\\ComputerVisionSolution\\flaskAPI_ML\\uploads\\User_Model_Files\\mynewmodel.hdf5\n",
      "Epoch 8/50\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.5647 - accuracy: 0.4223 - val_loss: 0.4473 - val_accuracy: 0.7647\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.39789\n",
      "Epoch 9/50\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.5056 - accuracy: 0.4379 - val_loss: 0.5062 - val_accuracy: 0.2353\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.39789\n",
      "Epoch 10/50\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 0.5200 - accuracy: 0.5154 - val_loss: 0.5256 - val_accuracy: 0.2353\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.39789\n",
      "Epoch 11/50\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.4958 - accuracy: 0.5311 - val_loss: 0.5606 - val_accuracy: 0.2353\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.39789\n",
      "Epoch 12/50\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 0.5104 - accuracy: 0.5272 - val_loss: 0.5755 - val_accuracy: 0.2353\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.39789\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cc86571670>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test),callbacks=[early_stop,checkpoint])\n",
    "# model.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test) , callbacks=[checkpoint]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "model1 = keras.models.load_model(os.path.join(os.getcwd(),'uploads\\\\User_Model_Files\\\\classification demo-9.hdf5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python\\ttl\\lib\\site-packages\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.predict_classes(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
